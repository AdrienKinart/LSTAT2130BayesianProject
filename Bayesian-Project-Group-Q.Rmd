---
# You need to knit from "Project Directory"
# Document information

title: "LSTAT2130 - Bayesian Statistics"
subtitle: "Project - Group Q"

authors:
  - "Lionel Lamy - 1294-1700"
  - "Adrien Kinart"
  - "Simon Lengendre"

# If multiple authors: - "Lionel Lamy - 1294-1700" is prettier.

# ---

# Logo cant have special char in the path as underscore
logo: "resources/img/UCLouvainLogoSciences.jpg"

institute: "Université catholique de Louvain"
faculty: "Louvain School of Statistics"
# department: ""

context: ""
date: \today

# ---
colorlinks: false
bordercolorlinks: true

linkcolor: "black"
urlcolor:  "black"
citecolor: "blue"

linkbordercolor: "black"
urlbordercolor: "black"
citebordercolor: "blue"

links-as-notes: false
# ---
# header-includes: 
#   -
output:
  pdf_document:
    template: template/markdown.tex
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  echo = F,
  eval = T,	
  	
  warning = F,	
  message = F,
  
  	
  out.width= "80%",
  fig.align = "center",	
  fig.path = "resources/figs/",
  
  tidy=TRUE,
  tidy.opts=list(width.cutoff=70)
)	
```

# Introduction


```{r}
library(EnvStats)
library(mnormt)
library(coda)
```

```{r matrix_setup}
Table <- matrix(data= c(25,69, 65,106, 80,106,136,94,76,46,
                       17,36, 47,58 , 47,53 ,59 ,54,33,21),
                nrow = 2, byrow = T)
rownames(Table) <- c("Flanders", "Wallonia") 
colnames(Table) <- c("<1200", "[1200-1500)", "1500-1800", "1800-2300", "2300-2700",
                     "2700-3300", "3300-4000", "4000-4900", "4900-6000", ">6000")
Intervals <- c(1200,1500,1800, 2300,2700,3300,4000,4900,6000)

NbFlemish <-  sum(Table[1,])
NbWaWalloons <-sum(Table[2,])  
```




```{r functions_setup}
kappaFct <- function(phi){1/phi}
lambdaFct <- function(phi,mu){1/(phi*mu)}
```

```{r flanders_wallonia}
# Flanders
Fl <- c(rep(1200,25), rep(1350,69), rep((1500+1800)/12,65), rep((1800+2300)/2,106),rep((2300+2700)/2,80), rep(3000,106),
        rep((3300+4000)/2,136), rep(4450,94),rep((4900+6000)/2,76), rep(6000,46))

Estimation_Fl <-  egamma(Fl)
Estimated_kappa_Fl <- Estimation_Fl$parameters["shape"]
Estimated_lambda_Fl <- 1/Estimation_Fl$parameters["scale"]
Estimated_mu_Fl <- Estimated_kappa_Fl/Estimated_lambda_Fl
Estimated_mu_Fl
Estimated_phi_Fl <- 1/Estimated_kappa_Fl
Estimated_phi_Fl

estimGamma_Fl <- rgamma(10000,Estimated_kappa_Fl,Estimated_lambda_Fl)
hist(Fl, probability = T, xlim = c(-500,8000))
curve(dgamma(x,Estimated_kappa_Fl,Estimated_lambda_Fl), add = TRUE)

# Wallonia
Wall <- c(rep(1200,17), rep(1350,36), rep((1500+1800)/12,47), rep((1800+2300)/2,58),rep((2300+2700)/2,47), rep(3000,53),
        rep((3300+4000)/2,59), rep(4450,54),rep((4900+6000)/2,33), rep(6000,21))


Estimation_Wall <-  egamma(Wall)
Estimated_kappa_Wal <- Estimation_Wall$parameters["shape"]
Estimated_lambda_Wal <- 1/Estimation_Wall$parameters["scale"]
Estimated_mu_Wal <- Estimated_kappa_Wal/Estimated_lambda_Wal
Estimated_mu_Wal
Estimated_phi_Wal <- 1/Estimated_kappa_Wal
Estimated_phi_Wal
estimGamma_Wal <- rgamma(10000,Estimated_kappa_Wal,Estimated_lambda_Wal)
hist(Wall, probability = T, xlim = c(-500,8000))
curve(dgamma(x,Estimated_kappa_Wal,Estimated_lambda_Wal), add = TRUE)

```



```{r JustAnExample}
muExample <- 2500; phiExample <- 1
kappaExample <- kappaFct(phiExample)
lambdaExample <- lambdaFct(muExample,phiExample)

pgamma(2700,kappaExample,lambdaExample)-pgamma(2300,kappaExample,lambdaExample)
dgamma(2700, kappaExample, lambdaExample)
```

# Question 1
Let $\theta_k:= (\mu_k, \phi_k)$ be the set of parameters for a HNI with respect to region $k$.

## (a) Theoretical probability

Let $X$ be the monthly net income of 1123 Belgian households net income (HNI) older than 30 years. Regardless the 2 regions ($k=\{1,2\}$ wrt Flanders and Wallonnia, respectively), is assumed it follows a Gamma distribution. It can be reparametrised in terms of its mean $\mu$ and dispersion parameter $\phi$ with the following trick: 


$$
\begin{split}
\text{shape: } \kappa & = \frac{1}{\phi} \\
\text{rate: } \lambda & = \frac{1}{\phi\; \mu}
\end{split}
$$


For both regions $k=\{1,2\}$: This gives


$$
f(x_k) = \frac{(\phi_k\mu_k)^{-1/\phi_k}}{\Gamma(\phi_k^{-1})} \
x_k^{1/\phi_k-1} \exp{(\frac{-x_k}{\phi_k \mu_k})}
$$

Then, the probability to fall into a certain HNI interval $I$ is: 

$$
P( x_k \in I_j) =\int_{I_j} \frac{(\phi_k \mu_k)^{-1/\phi_k}}{\Gamma(\phi_k^{-1})} x_k^{1/\phi_k-1} \exp{(\frac{-x_k}{\phi_k \mu_k})}\ \diff{x}
$$


Using CDF writing, for a region $k$ , 

$$
p_j =
\begin{cases}
 F(x_{j}, \kappa, \lambda) \ \text{if j =1}\\
 F(x_{j+1}, \kappa, \lambda) - F(x_{j}, \kappa, \lambda) \ \text{if} \ j \in \{2,..9\}\\
 1 - F(x_{j}, \kappa, \lambda) \ \text{if j =10}\\
\end{cases}
$$

$$
p_j =
\begin{cases}
 \frac{1}{\Gamma(\kappa)} \gamma(\kappa, \lambda x_j)  \ \text{if j =1}\\
  \frac{1}{\Gamma(\kappa)} \Big(\gamma(\kappa, \lambda x_{j+1}) - \gamma(\kappa, \lambda x_j)  \Big)   \ \text{if} j \in \{2,..9\}\\
 1- \frac{1}{\Gamma(\kappa)} \gamma(\kappa, \lambda x_j)  \ \text{if j =10}\\
\end{cases}
$$
In terms of $\mu$ and $\phi$
$$
p_j =
\begin{cases}
 \frac{1}{\Gamma(\phi^{-1})} \gamma(\phi^{-1}, (\mu \phi)^{-1} x_j)  \ \text{if j =1}\\
  \frac{1}{\Gamma(\phi^{-1})} \Big(\gamma(\phi^{-1}, (\mu \phi)^{-1}x_{j+1}) - \gamma(\kappa, (\mu \phi)^{-1} x_j)  \Big)   \ \text{if} j \in \{2,..9\}\\
 1- \frac{1}{\Gamma(\phi^{-1})} \gamma(\phi^{-1}, (\mu \phi)^{-1} x_j)  \ \text{if j =10}\\
\end{cases}
$$


$$
p_j =
\begin{cases}
 \frac{1}{\Gamma(\phi^{-1})} \int^{(\mu \phi)^{-1} x_j}_0 t^{\phi^{-1}-1} e^{-t} \diff{t}   \ \text{if j =1}\\
  \frac{1}{\Gamma(\phi^{-1})} \int^{(\mu \phi)^{-1}x_{j+1}}_{ (\mu \phi)^{-1} x_j} t^{\phi^{-1}-1} e^{-t} \diff{t}  \ \text{if} j \in \{2,..9\}\\
 \frac{1}{\Gamma(\phi^{-1})} \int^{+ \infty}_{(\mu \phi)^{-1}x_j} t^{\phi^{-1}-1} e^{-t} \diff{t}  \ \text{if j =10}\\
\end{cases}
$$




## (b) Theoretical expression for the likelihood
On behalf of writing simplicity, the region index is removed. Since the frequency distribution in a given region is multinomial, i.e. 

We have, writing $P:=(p{1},..,p_{10})$ and $X:= (X_{1},... X_{10})$:

$$
\begin{split}
X | P  \sim \text{Mul}(N,P)  & = \frac{x!}{x_{1}! \, ... \, x_{10}! } p_1^{x_1} \times ... \times p_{10}^{x_{10}} \; \text{when} \sum_{j=1}^{10} p_j = 1 \\
&=0 \; \text{otherwise}
\end{split}
$$

Up to a multiplicative constant, the likelihood can be written as follow:

$$
\begin{split}
L(\theta_k, D_k) = P (D_k | \mu_k, \phi_k) \propto \prod_{j=1}^{10} p_{k,j}^{x_{k,j}}
\end{split}
$$

## Taking approximation 

$p_j$ corresponds to the area in the $j^{\text{th}}$ interval. One can take the approximation mean the mean,e.g. $x_{Flanders,3} = (1500+1800)/2 = 1650$ ? On can approximate that with $f(x_i) \Delta$ where $\Delta$ is the unit of measurement. 

$$
\begin{split}
p_j =P(x_j - \frac{\Delta_j}{2} < x_j <x_j + \frac{\Delta_j}{2} ) & \approx f(x_j) \Delta_j \\
& \approx \frac{1}{\Gamma(\phi_k^{-1})} \big( \phi_k \mu \big)^{-1/\phi_k} x_j^{\frac{1}{\phi_k}-1} \exp{(\frac{-x_j}{\phi_k \mu})} \Delta_j
\end{split}
$$
This gives for the likelihood: 


$$
\begin{split}
P (D | \mu, \phi) & \propto \prod_{j=1}^{10}  x_j^{\frac{1}{\phi_k}-1} \exp{(\frac{-x_j}{\phi_k \mu})} \Delta_j \\
& \propto \exp{(\frac{- \sum x_j}{\phi_k \mu})}  \prod_{j=1}^{10}  x_j^{\frac{1}{\phi_k}-1}  \Delta_j \\
\end{split}
$$

## Not taking approximation but the CDF differences


$$
\begin{split}
P(D| \kappa, \lambda) & \propto \big(\frac{1}{\Gamma(\kappa)}\big)^{\sum_{i=1} x_j} \gamma(x_1 , \kappa, \lambda)^{x_1} \Bigg[ \prod_{j=2}^9  \Big( \gamma(x_{j} , \kappa, \lambda)- \gamma(x_{j-1} , \kappa, \lambda)\Big)^{x_j}\Bigg] \big(1-\gamma(x_{10} , \kappa, \lambda)\big)^{x_{10}}  \\
& \propto \big(\frac{1}{\Gamma(\kappa)}\big)^{\sum_{i=1} x_j}  \Bigg( \int_0^{\lambda x_1}  x^{\kappa-1} e^{-x} \diff x \Bigg)^{x_1}  \Bigg[ \prod_{j=2}^9 \Big( \int_{\lambda  x_j-1}^{\lambda x_{j}}  x^{\kappa-1} e^{-x} \diff x \Big)^{x_{j}}\Bigg] \Bigg( \int_{\lambda x_{10}}^{+\infty}  x^{\kappa-1} e^{-x} \diff x \Bigg)^{x_{10}}
\end{split} 
$$

If we write $[0; x_1], [x_1; x_2], ..., [x_9; x_{10}], [x_{10}, +\infty]$ as $I_1, I_2, ..., I_9$ and $I_{10}$. The notation can be lightened. With respect to the region $k$, this gives:
$$
P(D_k | \kappa_k, \lambda_k) \propto \big(\frac{1}{\Gamma(\kappa)}\big)^{\sum_{i=1} x_j}  \prod_{j=1}^{10} \Bigg( \int\limits_{\lambda_k I_j} x^{\kappa_k-1} e^{-x} \diff x \Bigg)^{x_{k,j}}
$$

Writing in terms of $\mu_k$ and $\phi_k$: 

$$
P(D_k | \mu_k, \phi_k) \propto  \prod_{j=1}^{10} \Bigg( \frac{1}{\Gamma(\phi^{-1})} \int\limits_{ \frac{I_j}{\phi_k \mu_k}} x^{(\phi_k^{-1}-1)} e^{-x} \diff x \Bigg)^{x_{k,j}}
$$

## NOT taking approximation but with PDF definitions: 



# Question 2 : Priors 

- Statement 1: we are at 95% convinced that the mean net monthly household income in a given region is in the interval (2400, 3600). If one assumes a normal distribution for the mean $\mu_k$ (à justifier), then it is possible to get a prior of the distriubtion for both regions: 

For both regions $\mu_0 = 3000$. Then, to get the standard deviation: 

$$
\begin{split}
& 3000 - t_{(n_k-1, 1-\alpha/2)} \frac{s_k}{\sqrt{n_k}} = 2400 \\
\rightarrow  & \hat{\sigma}_0 = \frac{s_k}{\sqrt{n_k}}= \frac{600}{t_{(n_k-1, 1-\alpha/2)}}
\end{split}
$$

```{r}
mu_prior <- 3000
sigma_prior <- 306
```


$\hat{\sigma}_{Fl} = 306$

So we have 
$$\mu \sim N(\mu_0= 3000, \sigma_0 =306)$$

$$
\begin{split}
 \pi(\mu) \propto \sigma_0^{-1/2} \exp{ \big(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\big) }
\end{split}
$$


- dispersion parameter: 

If one considers that the parameter can be in any point within the interval (0.0, 10.0) with the same probability, then one could say that it follows a uniform distribution between those to interval 

$$
\phi_k \sim U (a=0,b=10) \propto 1_{0,10}
$$

Using the entropy theorem, the conjugate prior would be the product of the two last quantity: 


$$
\begin{split} 
\text{prior:}  P(\mu_k, \phi_k) & = P(\mu_k) \ P(\phi_k) \\
& \propto \sigma_0^{-1/2} \exp{ \big(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\big) } 1_{0;10}
\end{split}
$$

# Question 3

## Question 3a: posterior

### With approximation 
$\sigma^2 = \phi \mu^2$

$$
\begin{split}
P(\mu, \phi | D) & \propto \exp{(\frac{- \sum x_j}{\phi_k \mu})}  \prod_{j=1}^{10}  x_j^{\frac{1}{\phi_k}-1}  \Delta_j \sigma^{-1/2} \exp{ \big(-\frac{1}{2\sigma^2}(\bar{x}_k-\mu)^2\big) } 1_{0,10} \\ 
& \propto \exp{(\frac{- \sum x_j}{\phi_k \mu})}  \prod_{j=1}^{10}  x_j^{\frac{1}{\phi_k}-1}  \Delta_j (\frac{1}{\sqrt{\phi} \mu}) \exp{ \big(-\frac{1}{2 \phi \mu^2}(\bar{x}_k-\mu)^2\big) } 1_{0,10} \\
\end{split}
$$


### Without approximation

$$
\begin{split}
P(\mu_k, \phi_k | D) \propto  \Bigg( \prod_{j=1}^{10} \Bigg( \frac{1}{\Gamma(\phi^{-1})} \int\limits_{ \frac{I_j}{\phi_k \mu_k}} x^{(\phi_k^{-1}-1)} e^{-x} \diff x \Bigg)^{x_{k,j}} \Bigg)  \sigma_0^{-1/2} \exp{ \big(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\big) } 1_{0;10}
\end{split}
$$

## 3.b 

the log likelihood is, up to an additive constant:
$$
\begin{split}
l(\mu,\phi) & \propto  \sum_{j=1}^{10} x_j \ln{\Bigg(\frac{1}{\Gamma(\phi^{-1})} \int\limits_{ \frac{I_j}{\phi_k \mu_k}} x^{(\phi_k^{-1}-1)} e^{-x} \diff x \Bigg)}
\end{split}
$$
so the log-posterior is: 

$$
h(\mu,\phi) \propto  l(\mu,\phi) + \ln(\pi(\mu,\phi))
$$

$$
h(\mu,\phi) = C^t +  \sum_j x_j \ln{\big(F(\mu,\phi,x_{j+1})-F(\mu,\phi,x_{j})  \big)} - \frac{1}{2 \sigma_0^2} (\mu-\mu_0)^2
$$



```{r}
lpost <- function(theta,freq){
  Intervals <- c(1200,1500,1800, 2300,2700,3300,4000,4900,6000) # length =9 
  mu_prior <- 3000
  sigma_prior <- 306
  
  mu <- theta[1]
  phi <- theta[2]
  kappa <- 1/phi
  lambda <- 1/(phi*mu)
  n <- length(Intervals)

  #initialisation: j=1 ==>  x_1 * proba(0,x_1) 
  LL <- freq[1]* log(pgamma(Intervals[1], kappa, lambda))
  
  for (i in 2:(n-1)) {
    LL <- LL + freq[i] *log(pgamma(Intervals[i+1], kappa, lambda) - pgamma(Intervals[i], kappa, lambda))
  }
  ## de x_n  à +infinity (=> 6000 --> inf )
  LL <-  LL+ freq[n]* log(1-pgamma(Intervals[n], kappa, lambda))
  logPost <- LL - 1/2*(mu-mu_prior)^2/sigma_prior^2
  logPost
}
```

```{r}
lpost(c(Estimated_mu_Fl, Estimated_phi_Fl), t(Table[1,]))
lpost(c(Estimated_mu_Wal, Estimated_phi_Wal), t(Table[2,]))
```


# 4. 

## Estimation 
Start from the log-posterior $h(\mu_k, \phi_k)$. One assumes the distribution is unimodale then, if it is normally distributed, then the mode=mean, which means that the mean can be found by optimizing the log-likelihood with respect to its parameters. (aussi assumer $\mu$ et $\phi$ indépendant?? ), then blablabla (expliquez avec hessienne). 


Based on http://www.sumsar.net/blog/2013/11/easy-laplace-approximation/#:~:text=Laplace%20Approximation%20of%20Posterior%20Distributions&text=Laplace%20approximation%20is%20a%20method,the%20posterior%20at%20the%20mode. 

```{r}
inits <- c(mu = mu_prior , phi = 0.01)
```

### Flanders
```{r}

fit_Fl <- optim(inits, lpost, control = list(fnscale = -1), hessian = TRUE, freq= Table[1,] )
param_mean_Fl <- fit_Fl$par
param_cov_mat_Fl <- solve(-fit_Fl$hessian)
round(param_mean_Fl, 2)
round(param_cov_mat_Fl, 3)
```

```{r}
library(mvtnorm)
samples_Fl <- rmvnorm(10000, param_mean_Fl, param_cov_mat_Fl)
plot(density(samples_Fl[,1]), main = 'laplace approximation for mu in Flanders')
plot(density(samples_Fl[,2]), main = 'laplace approximation for phi in Flanders')
```


### Wallonia

```{r}
fit_Wal <- optim(inits, lpost, control = list(fnscale = -1), hessian = TRUE, freq= Table[2,] )
param_mean_Wal <- fit_Wal$par
param_cov_mat_Wal <- solve(-fit_Wal$hessian)
round(param_mean_Wal, 2)
round(param_cov_mat_Wal, 3)
```
```{r}
samples_Wal <- rmvnorm(10000, param_mean_Wal, param_cov_mat_Wal)
plot(density(samples_Wal[,1]), main = 'laplace approximation for mu in Wallonia')
plot(density(samples_Wal[,2]), main = 'laplace approximation for phi in Wallonia')
```

## Credible intervals Laplce approximation

```{r}
levels <- c(0.1,0.25)
```

### Flanders
The credible region is given here below: 

```{r}
plot(x =samples_Fl[,1] , y = samples_Fl[,2] , xlab = 'mu', ylab='phi', pch='.')
x1 <- seq(3400,4000,length=1000)
y1 <- seq(0.16,0.26, length=1000)
lpostFl_laplaceApprox <- function(MyMu,MyPhi){ #theta_l = c(mu,phi)
  MuPhi <-  cbind(MyMu,MyPhi)
  dmvnorm(MuPhi, colMeans(samples_Fl), cov(samples_Fl), log=TRUE)
}

z1 <- outer(x1,y1,lpostFl_laplaceApprox)
R <- exp(z1-max(z1)) 
lvls <- c(0.01, 0.25, 0.5, 0.75, 0.9)
contour(x1,y1, R, levels = exp(-0.5*qchisq(lvls,2)), add=T, lwd=2, labels = (1-lvls))

```

We needs to get the credible interval for $\mu$. This means that we needs the marginal posterior distribution: 

$$
P(\mu| D) \propto \int p(\mu, \phi |D) \diff \phi
$$
It can be shown that the marginal (univariate) distribution of the bivariate Gaussian distribution $N\big( \mu_\theta, \Sigma)$ with $\mu_\theta = (E(\mu), E(\phi) )$ and 

$$
\Sigma =
\begin{pmatrix}
\sigma_\mu & \sigma_{\mu,\phi} \\
\sigma_{\phi,\mu} & \sigma_\phi
\end{pmatrix}
$$ 
also follows a normal distribution. See \href{https://paolomaccallini.com/2018/06/20/bivariate-normal-distribution/}{here}. Hence, for $\mu$, one has: 

$$\mu|D \sim N(E(\mu),\sigma_\mu) $$
We needs that $95\%$ of the marginal posterior to fall into the interval for Flanders:
```{r}
alpha <- 0.05
marginal_posterior_mu_Fl <- rnorm(10000,param_mean_Fl[1], sqrt(param_cov_mat_Fl[1,1]))
# Quantile based credible intervals 
QuantileCI_Fl <- quantile(marginal_posterior_mu_Fl, p=c(alpha/1,1-alpha/2))
# HPD intervals 
HPDIntervals_Fl <- HPDinterval(as.mcmc(marginal_posterior_mu_Fl), prob = 1-alpha)

plot(density(samples_Fl[,1]), main = 'laplace approximation for mu in Flanders')
legend("topleft", legend=c("HPD", "quantile-based"),
       col=c("red", "green"), lty=1:2)

abline(v=c(HPDIntervals_Fl[1], HPDIntervals_Fl[2]), col="red", lty=1)
abline(v=c(QuantileCI_Fl[1], QuantileCI_Fl[2]), col="green", lty=2)
```
Il faut encore faire pour phi de la Flandre

# Question 5

## (a) 


```{r}
# Starting value for mu and and phi defined earlier ==> defined using MLE: (Estimated_mu_Fl,Estimated_phi_Fl)
# Sigma_hat to be used: the proposed one is the one from the Laplace approximation: cov(samples_Fl)
Sigma_hat_metropolis_Fl <- cov(samples_Fl)
M <- 21000
burnin <- 1000 # after, should become stationary
theta <- array(dim=c(2,M))
theta[,1] <- c(Estimated_mu_Fl, Estimated_phi_Fl) # starting values
# At each iteration, 1 column gonna be filled in. 
theta[, 1:5]

n_accept<-0 # it's a counter of the number of accepted samples
sd_prop<-2.6 # multiply the var-cov matric ==> influence the acceptance ratethat we want to be 23%
# ==> se trouve via iterations dans la boucle du bas jusqu'à avoir le bon acceptance rate. 

# iteration loop
for (i in 2:M){
  theta_prop <- theta[,i-1] + rmnorm(1,c(0,0),sd_prop^2*Sigma_hat_metropolis_Fl)
  prob <- min(1,exp(lpost(theta_prop, freq = t(Table[1,]) )-lpost(theta[,i-1], freq = t(Table[1,]))))
  accept <- (runif(1)<=prob)
  if (accept){
    theta[,i] <- theta_prop
    n_accept <- n_accept + 1
  } else {
    theta[,i] <- theta[,i-1]
  }
}
# Exclude burnin
theta = theta[,-c(1:burnin)]
rownames(theta) <- c("mu", "phi")

accept_rate <- paste0(round(n_accept/(M-1),digits=2)*100,"%")
cat("Acceptance rate : ",accept_rate,"\n") ## 22% ==> good :) (when sd_prop = l'autre alors 7%, c'es trop loin)


# Descriptive statistics for each model parameter
summary(t(theta))


```

Il va falloir comparer ça avec la Laplace approximation 


## (b) diagnostic 

difficult to say if mixing is good while checking the trace (?)
```{r}
par(mfrow=c(2,2))
traceplot(as.mcmc(t(theta)))
```

```{r}
acf((theta[1,]), lag.max = 30)
acf((theta[2,]), lag.max = 30)
effectiveSize(mcmc(t(theta)))
```

Assesment from a single chain

```{r}
geweke.diag(mcmc(t(theta)))
par(mfrow=c(2,1))
geweke.plot(mcmc(t(theta)), nbins = 20)
```

Except for one, they ye all into the confidence interval. Hence, there is no reason to think that the chain needs to be truncated or the chain to be made longer. 


## (c) Credible intervals for mu1
Bon là, y a tout en dessous

Metropolis: 
```{r}
# Metropolis HPD intervals
HPDmu_metrop_Fl<- HPDinterval(as.mcmc(t(theta)))[1,]
HPDphi_metrop_Fl<- HPDinterval(as.mcmc(t(theta)))[2,]
# quantile based Metropolis credible intervals
CImu_Fl <- quantile(theta[1,], probs = c(alpha/2,1-alpha/2))
CIphi_Fl <- quantile(theta[2,], probs = c(alpha/2,1-alpha/2))

densplot(as.mcmc((theta[1,])), col="blue", 
         main = "Estimated density of mu with metropolis algorithm")
abline(v=c(HPDmu_metrop_Fl[1], HPDmu_metrop_Fl[2]), col="red")
abline(v=CImu_Fl, col='green')

densplot(as.mcmc((theta[2,])), col="blue", 
         main = "Estimated density of phi with metropolis algorithm")
abline(v=c(HPDphi_metrop_Fl[1], HPDphi_metrop_Fl[2]), col="red")
abline(v=CIphi_Fl, col='green')
```


Comparison: voir les deux graphes et chopper les valeurs 

<!------>
\appendix

# Appendix{#appendix}
## Figures{#figures}
## Code{#code}

\bigskip
\begin{mdframed}[style=thicc, frametitle=Note, frametitlebackgroundcolor=black!30]
  For reproducibility purposes, the complete R project containing the source code and the results is available on \href{https://github.com/AdrienKinart/LSTAT2130BayesianProject}{github.com}.
\end{mdframed}


```{r appendix_pre}
# remotes::install_github('yihui/formatR')
library(formatR)
```

```{r appendix_code, eval=FALSE, echo=TRUE, ref.label=knitr::all_labels(appendix==T)}
```
