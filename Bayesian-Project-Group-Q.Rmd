---
# You need to knit from "Project Directory"
# Document information

title: "LSTAT2130 - Bayesian Statistics"
subtitle: "Project - Group Q"

authors:
  - "Adrien Kinart - xxxx-xxxx"
  - "Lionel Lamy - 1294-1700"
  - "Simon Lengendre - xxxx-xxxx"

# If multiple authors: - "Lionel Lamy - 1294-1700" is prettier.

# ---

# Logo cant have special char in the path as underscore
logo: "resources/img/UCLouvainLogoSciences.jpg"

institute: "Université catholique de Louvain"
faculty: "Louvain School of Statistics"
# department: ""

context: ""
date: \today

# ---
colorlinks: false
bordercolorlinks: true

linkcolor: "black"
urlcolor:  "black"
citecolor: "blue"

linkbordercolor: "black"
urlbordercolor: "black"
citebordercolor: "blue"

links-as-notes: false
# ---
# header-includes: 
#   -
output:
  pdf_document:
    template: template/markdown.tex
    toc: true
    toc_depth: 3
    
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  echo = F,
  eval = T,

  warning = F,
  message = F,


  out.width = "80%",
  fig.align = "center",
  fig.path = "resources/figs/",

  tidy = TRUE,
  tidy.opts = list(width.cutoff = 70)
)
```

# Introduction {.unlisted .unnumbered}

```{r}
# Require the necessary packages

if(!require(mnormt)){ install.packages("mnormt"); require(mnormt)}
if(!require(EnvStats)){ install.packages("EnvStats"); require(EnvStats)}
if(!require(R2WinBUGS)){ install.packages("R2WinBUGS"); require(R2WinBUGS)}
if(!require(coda)){ install.packages("coda"); require(coda)}
if(!require(rjags)){ install.packages("rjags"); require(rjags)}
```

<!--  -->

In this work, the Net Monthly Income (HNI) of household older than 30 years is studied across the two Belgian regions. These regions are denoted by \(k=\{1,2\}\) with respect to Flanders and Wallonia, respectively. The 1228 households were listed with respect to 10 income intervals. The detailed frequency table is given below:

\begin{table}[h]
\centering
\begin{tabular}{rccccccccccr}
\multicolumn{1}{c}{} &
  \multicolumn{10}{c}{Net monthly household income (in euros)} &
  \multicolumn{1}{c}{} \\ \cline{2-11}
    Region &
    \rotatebox{90}{<1200} &
    \rotatebox{90}{[1200, 1500)} &
    \rotatebox{90}{[1500,1800)} &
    \rotatebox{90}{[1800,2300)} &
    \rotatebox{90}{[2300,2700)} &
    \rotatebox{90}{[2700,3300)} &
    \rotatebox{90}{[3300,4000)} &
    \rotatebox{90}{[4000,4900)} &
    \rotatebox{90}{[4900,6000)} &
    \rotatebox{90}{$\ge$6000} &
    Total \\ \hline
    Flanders & 25 & 69 & 65 & 106 & 80 & 106 & 136 & 94 & 76 & 46 & 803 \\
    Wallonia & 17 & 36 & 47 & 58  & 47 & 53  & 59  & 54 & 33 & 21 & 425
\end{tabular}
\caption{Frequency table of the survey}
\end{table}

Let \(X\) be the HNI regardless the 2 regions , it is assumed it follows a Gamma distribution with parameters $\kappa$ and $\lambda$. It can be reparametrised in terms of its mean \(\mu\) and dispersion parameter \(\phi\) with the following trick:

$$
\begin{split}
\text{shape: } \kappa & = \frac{1}{\phi} \\
\text{rate: } \lambda & = \frac{1}{\phi\; \mu}
\end{split}
$$
By assuming the gamma distribution, one can plot their experimental histogram and plot the theoretical density plot by estimating the shape and rate parameters with Maximum of Likelihood. This allows to get a first sight on the data behavior at hand.

\begin{table}[h]
\centering

\begin{tabular}{lllll}
& Flanders  & Wallonia &  &  \\ \cline{1-4}
$\hat\mu$  & 3089.944 & 2914.882 &  &  \\
$\hat\phi$ & 0.399687 & 0.471615 &  &  \\
\end{tabular}
\caption {Estimated $\mu$ and $\phi$ for Flanders and Wallonia}
\end{table}

```{r matrix_setup}
Table <- matrix(
  data = c(
    25, 69, 65, 106, 80, 106, 136, 94, 76, 46,
    17, 36, 47, 58, 47, 53, 59, 54, 33, 21
  ),
  nrow = 2, byrow = T
)
rownames(Table) <- c("Flanders", "Wallonia")
colnames(Table) <- c(
  "<1200", "[1200-1500)", "1500-1800", "1800-2300", "2300-2700",
  "2700-3300", "3300-4000", "4000-4900", "4900-6000", ">6000"
)
Intervals <- c(0, 1200, 1500, 1800, 2300, 2700, 3300, 4000, 4900, 6000, 1e10)

NbFlemish <- sum(Table[1, ])
NbWaWalloons <- sum(Table[2, ])
```

```{r functions_setup}
kappaFct <- function(phi) {
  1 / phi
}
lambdaFct <- function(phi, mu) {
  1 / (phi * mu)
}
```

```{r flanders_wallonia, fig.align="center", fig.height=6, fig.width=15}
library(latex2exp)
par(mfrow = c(1, 2))
# Flanders
Fl <- c(
  rep(1200, 25), rep(1350, 69), rep((1500 + 1800) / 2, 65), rep((1800 + 2300) / 2, 106), rep((2300 + 2700) / 2, 80), rep(3000, 106),
  rep((3300 + 4000) / 2, 136), rep(4450, 94), rep((4900 + 6000) / 2, 76), rep(6000, 46)
)

Estimation_Fl <- egamma(Fl)
Estimated_kappa_Fl <- Estimation_Fl$parameters["shape"]
Estimated_lambda_Fl <- 1 / Estimation_Fl$parameters["scale"]
Estimated_mu_Fl <- Estimated_kappa_Fl / Estimated_lambda_Fl

Estimated_phi_Fl <- 1 / Estimated_kappa_Fl

estimGamma_Fl <- rgamma(10000, Estimated_kappa_Fl, Estimated_lambda_Fl)
hist(Fl, probability = T, xlim = c(-500, 7000), main = TeX("(Histogram of Flanders and gamma distribution with parameters $\\hat{\\mu_{Fl}}$ and $\\hat{\\phi_{Fl}}$)"), xlab = "Flanders")
curve(dgamma(x, Estimated_kappa_Fl, Estimated_lambda_Fl), add = TRUE)

# Wallonia
Wall <- c(
  rep(1200, 17), rep(1350, 36), rep((1500 + 1800) / 12, 47), rep((1800 + 2300) / 2, 58), rep((2300 + 2700) / 2, 47), rep(3000, 53),
  rep((3300 + 4000) / 2, 59), rep(4450, 54), rep((4900 + 6000) / 2, 33), rep(6000, 21)
)


Estimation_Wall <- egamma(Wall)
Estimated_kappa_Wal <- Estimation_Wall$parameters["shape"]
Estimated_lambda_Wal <- 1 / Estimation_Wall$parameters["scale"]
Estimated_mu_Wal <- Estimated_kappa_Wal / Estimated_lambda_Wal
Estimated_phi_Wal <- 1 / Estimated_kappa_Wal
estimGamma_Wal <- rgamma(10000, Estimated_kappa_Wal, Estimated_lambda_Wal)
hist(Wall, probability = T, xlim = c(-500, 7000), main = TeX("(Histogram of Wallonia and gamma distribution with parameters $\\hat{\\mu_{Wal}}$ and $\\hat{\\phi_{Wal}}$)"), xlab = "Wallonia")
curve(dgamma(x, Estimated_kappa_Wal, Estimated_lambda_Wal), add = TRUE)
```

```{r JustAnExample}
muExample <- 2500
phiExample <- 1
kappaExample <- kappaFct(phiExample)
lambdaExample <- lambdaFct(muExample, phiExample)

# pgamma(2700,kappaExample,lambdaExample)-pgamma(2300,kappaExample,lambdaExample)
# dgamma(2700, kappaExample, lambdaExample)
```


\newpage

# Question 1
Let \(\theta_k:= (\mu_k, \phi_k)\) be the set of parameters for a HNI with respect to region \(k\).

## (a) Theoretical probability

As expressed earlier, the distribution of monthly net income is described by a distribution, in terms of $\mu$ and $\phi$, for both region $k=\{1,2\}$: this gives
  \vspace{-0em}
  \begin{align}
    f(x_k) = \frac{(\phi_k\mu_k)^{-1/\phi_k}}{\Gamma(\phi_k^{-1})} x_k^{1/\phi_k-1}         \exp{(\frac{-x_k}{\phi_k \mu_k})}
  \end{align}
    
The cumulative distribution function is defined as : 
  \begin{align*}
    F(x) = \int_0^x f(u)\ \diff{u} = \frac{\gamma(\phi^{-1}, \frac{x}{\phi\mu})}{\Gamma(\phi^{-1})}
  \end{align*}
    
where $\Gamma(a)$ and $\gamma(a,b)$ are the complete gamma and lower incomplete gamma functions, defined as:

  \vspace{-1.5em}
  \begin{align*}
    \gamma(a,b) & = \int_{0}^{b} t^{a-1} \exp(-t) \diff{t}  \\
    \Gamma(a) & = \gamma(a, \infty)
  \end{align*}

Then, the probability to fall into a certain HNI interval \(I\), for each region, is:
  \begin{align*}
    P( x \in I_j) =\int_{I_j} \frac{(\phi\mu)^{\frac{-1}{\phi}}}{\Gamma(\phi^{-1})}x^{\frac{1}{\phi}-1} \exp{(\frac{-x}{\phi\mu})}\ \diff{x}
  \end{align*}
    
Using CDF writing , the probability can be proposed as a difference of CDF :

\begin{equation*} 
    p_j =
    \left\{
    \begin{array}{lcl}
     F(x_{j}) & \text{if} & j =1\\
     F(x_{j+1}) - F(x_{j}) & \text{if} & j \in \{2,..,9\}\\
     1 - F(x_{j}) & \text{if} & j = 10\\
    \end{array}
    \right.
\end{equation*}
    
\vspace{+1.5em}
\begin{equation*} 
\label{eq: CDFdifferences}
    p_j =
    \left\{
    \begin{array}{lcl}
        \frac{\gamma(\phi^{-1}, \frac{x_j}{\phi\mu})}{\Gamma(\phi^{-1})} &  \text{if} & j =1\\
        \frac{1}{\Gamma(\phi^{-1})} \Big(\gamma(\phi^{-1}, \frac{x_{j_+1}}{\phi\mu}) - \gamma(\phi^{-1}, \frac{x_j}{\phi\mu})  \Big) & \text{if} & j \in \{2,..,9\}\\
        1- \frac{\gamma(\phi^{-1}, \frac{x_j}{\phi\mu})}{\Gamma(\phi^{-1})}  & \text{if} & j =10\\
    \end{array}
    \right.
    \end{equation*}
    
\newpage

## (b) Theoretical expression for the likelihood 

Since the frequency distribution in a given region is  assumed multinomial,

We have, writing \(P:=(p{1},..,p_{10})\) and \(Y:= (Y_{1},... Y_{10})\):
    \begin{align*}
    Y | P  \sim \text{Mul}(Y,P)  & = \frac{ (\sum{y_i})!}{y_{1}! \, ... \, y_{10}! }\  p_1^{y_1} \times ... \times p_{10}^{y_{10}} \; \text{when} \sum_{j=1}^{10} p_j = 1 \\
    &=0 \; \text{otherwise}
    \end{align*}
    
For such a distribution, the likelihood $L$ is well known. Indeed, up to a multiplicative constant, for a region $k$, it is given by:
    
\begin{equation*}
    L(P_k, Y_k) = P (Y_k | P_k) \propto \prod_{j=1}^{10} p_{k,j}^{y_{k,j}}
\end{equation*}

  To translate this likelihood in terms of $\mu_k$ and $\phi_k$, one can use the definition of the probability referring to the difference of CDF  (see equation \ref{eq: CDFdifferences}). By substituting $p_{k,j}$ with it and writing \([0; x_1], [x_1; x_2], ..., [x_9; x_{10}], [x_{10}, +\infty]\) as \(I_1, I_2, ..., I_{10}\),  the likelihood function becomes, up to a multiplicative constant, as follows: 

\begin{equation*}
L(\mu_k, \phi_k, Y_k) \propto  \prod_{j=1}^{10} \Bigg( \frac{1}{\Gamma(\phi^{-1})} \int\limits_{ \frac{I_j}{\phi_k \mu_k}} x_j^{(\phi_k^{-1}-1)} e^{-x} \diff x \Bigg)^{y_{k,j}}
\end{equation*}

and then taking the logarithm, we obtain the log-likelihood $\ell$ for a certain region:
    
\begin{equation*}
        \ell (\mu_k, \phi_k, Y_k) = \sum_{j=1}^{10}y_{k,j}\times \ln(p_{k,j})
\end{equation*}
    
where $y_{k,j}$ is the frequency of households in the interval j in the region k, and we recall that $p_{k,j}$ corresponds to the probability, or the area of the $j^{\text{th}}$ interval in the region k. Again, by substituting with the difference of CDF, the log-likelihood becomes:
    
\begin{equation} 
\label{eq:LogLikelihoodDefinition}
\ell (\mu_k, \phi_k, Y_k)= \sum_{j=1}^{10}y_{k,j}\times \ln\Bigg( \frac{1}{\Gamma(\phi_k^{-1})} \int\limits_{ \frac{I_j}{\phi_k \mu_k}} x^{(\phi_k^{-1}-1)} e^{-x} \diff x \Bigg)
\end{equation}


\newpage

# Question 2

## Prior for the mean

Prior to the study, the average net monthly household income, regardless the region, is believed to be within the interval (2400, 3600) on a 95\% confidence level. A convenient probability function that translates this belief is the Gaussian distribution with a mean at the center and standard deviation such that its $95\%$ confidence interval corresponds to the mentioned bonds. This belief is confirmed by Monte Carlo simulations. Indeed, if one generate a large amount of samples and repeatedly take its mean, one can determine an estimated density for its mean. It is symmetric without fat tails, which confirm the goodness of this assumption. The density is shown here below: 

\vspace{+3em}

```{r}
n <- 10000
moyenne <- numeric(n)
for (i in 1:n) {
  moyenne[i] <- mean(rgamma(1000, Estimated_kappa_Fl, Estimated_lambda_Fl), ylim = c(0, 0.0065))
}

hist(moyenne, probability = T, breaks = 30, main = "", xlab = "")
legend("topleft", legend = c("Estimated density", "Esimated normal density"), col = c("red", "blue"), lty = 1, cex = 0.8)
lines(density(moyenne), col = "red", lwd = 2)
curve(dnorm(x, mean = mean(moyenne), sd = sd(moyenne)), add = T, col = "blue", lwd = 2)
```

\vspace{+2em}
Hence, by assuming such a distribution for the prior, one can easily estimate its parameters. For both regions $\mu_0 = 3000$ (the center of the confidence interval). Then, to get the standard deviation, one can use the decomposition of a classical confidence interval for the mean. 

\vspace{-2em}
\begin{equation*}
\begin{split}
    & 3000 - t_{(n_k-1, 1-\alpha/2)} \frac{s_k}{\sqrt{n_k}} = 2400 \\
    \rightarrow  & \hat{\sigma}_0 = \frac{s_k}{\sqrt{n_k}}= \frac{600}{t_{(n_k-1, 1-\alpha/2)}} \\
    & \text{ where: } t_{n_1-1, 1-\alpha/2} \approx t_{n_2-1, 1-\alpha/2} \approx 1.96 
\end{split}
\end{equation*}



```{r}
mu_prior <- 3000
sigma_prior <- 306.12
```


Therefore $\hat{\sigma}_{Fl} \approx \hat{\sigma}_{Wal} \approx 306.12$, then $\mu_k \sim N(\mu_{0,k}= 3000, \sigma_{0,k} =306.12)$. So we get as a prior for the parameter $\mu$, up to a multiplicative constant: 

\begin{equation}
\begin{split}
 \pi(\mu_k) \propto  \exp{ \big(-\frac{1}{2\sigma_0^2}(\mu_k-\mu_0)^2\big) } \ \forall k\in\{1,2\} 
\end{split}
\end{equation}



## Prior for the dispersion parameter

It is certain that $\phi_k$ lies in the interval $(0.0, 10.0)$ but there is no knowledge on how the probability mass is broken down. In order to reflect this prior knowledge, one can assume that the dispersion parameter is uniformly distributed with lower and upper bonds of 0 and 10, respectively, i.e. $\phi_k \sim \mathcal{U} (0,10)$. Hence, up to a multiplicative constant, the prior probability can be represented via the indicator function as below:

\begin{equation*}
     \pi(\phi_k) \propto \mathbf{1}_{0,10} \ \forall k\in\{1,2\}
\end{equation*}


## The conjugate prior

Since there is no prior information on the dependence structure of the priors, it is recommended to consider them as independent in order to remain as least-informative as possible( this follows the maximal entropy principle). By doing so, the conjugate prior becomes: 

\vspace{-2em}
\begin{equation} 
\label{eq: ConjugatePriorDefinition}
\begin{split} 
  \pi (\mu_k, \phi_k) & = \pi(\mu_k) \ \pi (\phi_k) \\
& \propto  \exp{ \big(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\big) } \mathbf{1}_{0;10}
\end{split}
\end{equation}

# Question 3

## (a) Joint posterior for Flanders

Since the likelihood and prior for $\mu_k$ and $\phi_k$ have been defined, a joint posterior can be expressed as their product. The joint posterior for Flanders, i.e. of $\theta_1 = (\mu_1,\phi_1)$ is defined, up to a multiplicative constant, here below:

\begin{equation*} 
\label{eq: JointPosteriorFlanders}
  \begin{split}
    P(\mu_1, \phi_1 | Y_1) \propto  \Bigg( \prod_{j=1}^{10} \Bigg( \frac{1}{\Gamma(\phi_1^{-1})} \int\limits_{ \frac{I_j}{\phi_1 \mu_1}} x^{(\phi_1^{-1}-1)} e^{-x} \diff x \Bigg)^{y_{1,j}} \Bigg)   \exp{ \big(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\big) } 1_{0;10}
  \end{split}
\end{equation*}

## (b) Write function lpost

Computing the logarithm of this expression, we find the log likelihood posterior, which is, up to an additive constant:

\begin{equation*}  
  h(\mu_1,\phi_1, Y_1) = C^t +  \ell(\mu_1, \phi_1, Y_1)+ \ln(\pi(\mu_1,\phi_1))
\end{equation*}
    
Replacing the log-likelihood and log of the prior by their expression (see Equations \ref{eq:LogLikelihoodDefinition} and \ref{eq: ConjugatePriorDefinition} ), the log-posterior can be theoretically written as below:

\begin{equation} 
\label{eq:LogPosteriorDefinition}
        h(\mu_1,\phi_1, Y_1) = C^t + \sum_{j=1}^{10}y_{1,j}\times \ln\Bigg( \frac{1}{\Gamma(\phi^{-1})} \int\limits_{ \frac{I_j}{\phi_1 \mu_1}} x^{(\phi_1^{-1}-1)} e^{-x} \diff x \Bigg) - \frac{1}{2 \sigma_{0}^2} (\mu-\mu_{0})^2 + \ln(\mathbf{1}_{0;10})
\end{equation}
    
This expression has been implemented in R by not considering the constant term.

```{r}
lpostold <- function(theta, freq) {
  Intervals <- c(0, 1200, 1500, 1800, 2300, 2700, 3300, 4000, 4900, 6000, Inf) # length =11. J'ai rajouté INFINI
  mu <- theta[1]
  phi <- theta[2]
  kappa <- 1 / phi
  lambda <- 1 / (phi * mu)
  n <- length(freq)
  # intialisation
  LL <- 0

  for (i in 1:n) {
    LL <- LL + freq[i] * log(pgamma(Intervals[i + 1], kappa, lambda) - pgamma(Intervals[i], kappa, lambda))
  }

  logPost <- LL + dnorm(mu, mean = mu_prior, sd = sigma_prior, log = T) + dunif(phi, min = 0, max = 10, log = T) # /!\ mu_prior et sigma_prior doivent avoir été determiné plus haut
  return(logPost)
}
```


```{r, echo=T}
# Intervals of the frequency table
intervals <- c(0, 1200, 1500, 1800, 2300, 2700, 3300, 4000, 4900, 6000, Inf)

# Utility function to compute the prob of being between high and low
pgammadiff <- function(low, high, kappa, lambda) {
  pgamma(high, kappa, lambda) - pgamma(low, kappa, lambda)
}

# Utility fonctions (already defined in the setup)
kappa <- function(phi) {
  1 / phi
}
lambda <- function(phi, mu) {
  1 / (phi * mu)
}

mu_prior <- 3000
sigma_prior <- 306.12
lpost <- function(theta, freq) {
  # Transform mu and phi -> kappa and lambda
  kappa <- kappa(theta[2])
  lambda <- lambda(theta[2], theta[1])

  # Likelihood : sum_j^n[x_j * ln(probability_of_being_in_interval_j)]
  LL <- sum(sapply(1:length(freq), function(j) {
    freq[j] * log(pgammadiff(low = intervals[j], high = intervals[j + 1], kappa, lambda))
  }))

  # Log posterior
  lpi <- dnorm(theta[1], mu_prior, sigma_prior, log = T) + dunif(theta[2], 0, 10, log = T)
  lpost <- LL + lpi

  names(lpost) <- "lpost"
  return(lpost)
}
```

\newpage

# Question 4

## Estimation 
Start from the log-posterior $h(\mu_k, \phi_k)$. One assumes the distribution is unimodale then, if it is normally distributed, then the mode=mean, which means that the mean can be found by optimizing the log-likelihood with respect to its parameters. (aussi assumer $\mu$ et $\phi$ indépendant?? ), then blablabla (expliquez avec hessienne). 


Based on http://www.sumsar.net/blog/2013/11/easy-laplace-approximation/#:~:text=Laplace%20Approximation%20of%20Posterior%20Distributions&text=Laplace%20approximation%20is%20a%20method,the%20posterior%20at%20the%20mode. 


### Flanders {-}
```{r}
inits_Fl <- c(mu = Estimated_mu_Fl, phi = Estimated_phi_Fl)
fit_Fl <- optim(inits_Fl, lpost, control = list(fnscale = -1), hessian = TRUE, freq = Table[1, ])
param_mean_Fl <- fit_Fl$par
param_cov_mat_Fl <- solve(-fit_Fl$hessian)
round(param_mean_Fl, 2)
round(param_cov_mat_Fl, 3)
```

```{r}
library(mvtnorm)
laplace_samples_Fl <- rmvnorm(100000, param_mean_Fl, param_cov_mat_Fl)
plot(density(laplace_samples_Fl[, 1]), main = "laplace approximation for mu in Flanders")
plot(density(laplace_samples_Fl[, 2]), main = "laplace approximation for phi in Flanders")
```


### Wallonia {-}

```{r}
inits_Wal <- c(mu = Estimated_mu_Wal, phi = Estimated_phi_Wal)
fit_Wal <- optim(inits_Wal, lpost, control = list(fnscale = -1), hessian = TRUE, freq = Table[2, ])
param_mean_Wal <- fit_Wal$par
param_cov_mat_Wal <- solve(-fit_Wal$hessian)
round(param_mean_Wal, 2)
round(param_cov_mat_Wal, 3)
```
```{r}
samples_Wal <- rmvnorm(10000, param_mean_Wal, param_cov_mat_Wal)
plot(density(samples_Wal[, 1]), main = "laplace approximation for mu in Wallonia")
plot(density(samples_Wal[, 2]), main = "laplace approximation for phi in Wallonia")
```

## Credible intervals Laplce approximation

```{r}
levels <- c(0.1, 0.25)
```

### Flanders {-}
The credible region is given here below: 

```{r}
plot(x = laplace_samples_Fl[, 1], y = laplace_samples_Fl[, 2], xlab = "mu", ylab = "phi", pch = ".")
x1 <- seq(3400, 4000, length = 1000)
y1 <- seq(0.16, 0.26, length = 1000)
lpostFl_laplaceApprox <- function(MyMu, MyPhi) { # theta_l = c(mu,phi)
  MuPhi <- cbind(MyMu, MyPhi)
  dmvnorm(MuPhi, colMeans(laplace_samples_Fl), cov(laplace_samples_Fl), log = TRUE)
}

z1 <- outer(x1, y1, lpostFl_laplaceApprox)
R <- exp(z1 - max(z1))
lvls <- c(0.01, 0.25, 0.5, 0.75, 0.9)
contour(x1, y1, R, levels = exp(-0.5 * qchisq(lvls, 2)), add = T, lwd = 2, labels = (1 - lvls), col = "red")
```
```{r}
library(MASS)
library(plotly)
den3dLaplaceApprox <- kde2d(laplace_samples_Fl[, 1], laplace_samples_Fl[, 2])
plot_ly(x = den3dLaplaceApprox$x, y = den3dLaplaceApprox$y, z = den3dLaplaceApprox$z) %>% add_surface()
```


We needs to get the credible interval for $\mu$. This means that we needs the marginal posterior distribution: 

$$
P(\mu| D) \propto \int p(\mu, \phi |D) \diff \phi
$$
It can be shown that the marginal (univariate) distribution of the bivariate Gaussian distribution $N\big( \mu_\theta, \Sigma)$ with $\mu_\theta = (E(\mu), E(\phi) )$ and 

$$
\Sigma =
\begin{pmatrix}
\sigma_\mu & \sigma_{\mu,\phi} \\
\sigma_{\phi,\mu} & \sigma_\phi
\end{pmatrix}
$$ 
also follows a normal distribution. See \href{https://paolomaccallini.com/2018/06/20/bivariate-normal-distribution/}{here}. Hence, for $\mu$, one has: 

$$\mu|D \si= N(E(\mu),\sigma_\mu) $$
We needs that $95\%$ of the marginal posterior to fall into the interval for Flanders:
```{r}
alpha <- 0.05
marginal_posterior_mu_Fl <- rnorm(10000, param_mean_Fl[1], sqrt(param_cov_mat_Fl[1, 1]))
# Quantile based credible intervals
QuantileCI_Laplace_Fl <- quantile(marginal_posterior_mu_Fl, p = c(alpha / 1, 1 - alpha / 2))
# HPD intervals
HPDIntervals_Laplace_Fl <- HPDinterval(as.mcmc(marginal_posterior_mu_Fl), prob = 1 - alpha)

plot(density(laplace_samples_Fl[, 1]), main = "laplace approximation for mu in Flanders")
legend("topleft",
  legend = c("HPD", "quantile-based"),
  col = c("red", "green"), lty = 1:2
)

abline(v = c(HPDIntervals_Laplace_Fl[1], HPDIntervals_Laplace_Fl[2]), col = "red", lty = 1)
abline(v = c(QuantileCI_Laplace_Fl[1], QuantileCI_Laplace_Fl[2]), col = "green", lty = 2)
```

# Question 5

## (a) Random walk  component-wise Metropololis algorithm
One first needs starting values for $\theta_t := (\mu_t,\phi_t)$. One can take advantage of the MLE estimations of $\kappa$ and $\lambda$ and compute $\mu_0$ and $\phi_0$ using the relations described earlier. At each iteration, a new candidate will be proposed. To build it, it is a good idea to use the variance-covariance matrix computed into the Laplace approximation section since it should, more or less, describe the standard deviations of the parameters of interest $\hat{\sigma}_\mu$ and$\hat{\sigma}_\phi$. A factor, $f_1$ and $f_2$ should multiply their standard deviation in order to optimize the acceptance rate (goal is $40 \%$). Moreover, since the posterior probability is on a log-scale, the comparison of probabilities should be made taking the exponential of the difference of the candidate with the current state. In this work, $10\%$ of the generated data is considered as burn-in.

That being, this gives the following algorithm:

1. $t=0$: Set starting values of the parameters of interest to their MLE: $\theta_0 := (\hat{\mu}_{MLE},\hat{\phi}_{MLE})$
2. for $t=2,3,.. 55 000$    
    + Draw $\mu_{\text{prop}}$ from a normal distribution centered on value derived at $t-1$, i.e.  $\mu_{\text{prop}} \sim N(\mu_{t-1} ,f_1 \hat{\sigma}_\mu)$. The candidate is then $\theta^\mu_{prop} := (\mu_{\text{prop}}, \phi_{t-1} )$
    + Compute $prob_\mu= \min{\Big(1,\exp\big(h(\theta^\mu_{prop})- h(\theta_{t-1}\big) \Big)}$
    + Set $\mu_t = \mu_{\text{prop}}$ with probability $prob_\mu$, $\mu_t = \mu_{t-1}$ otherwise.
    + Draw $\phi_{\text{prop}}$ from a normal distribution centered on value derived at $t-1$, i.e.  $\phi_{\text{prop}} \sim N(\phi_{t-1} ,f_2 \hat{\sigma}_\phi)$. The candidate is then $\theta^\phi_{prop} := (\mu_{t-1}, \phi_{\text{prop}})$
    + Compute $prob= \min{\Big(1,\exp\big(h(\theta^\phi_{prop})- h(\theta_{t-1}\big) \Big)}$
    + Set $\phi_t = \phi_{\text{prop}}$ with probability $prob_\phi$, $\phi_t = \phi_{t-1}$ otherwise.
    + Set $\theta_t = (\mu_t, \phi_t)$


```{r}
metropolis_algorithm <- function(M, theta, sd.propositions, factors, frequencies) {
  theta <- as.vector(theta)
  sd.propositions <- as.vector(sd.propositions)
  factors <- as.vector(factors)
  frequencies <- as.vector(frequencies)

  thetas <- array(dim = c(M + 1, 2))
  thetas[1, ] <- theta

  accepted <- c(0, 0)

  sigma_mu <- factors[2] * sd.propositions[1]
  sigma_phi <- factors[2] * sd.propositions[2]

  for (i in 2:(M + 1)) {
    theta_mu <- theta_phi <- this_theta <- thetas[i - 1, ]

    theta_mu[1] <- this_theta[1] + rnorm(1, 0, sigma_mu)
    theta_phi[2] <- this_theta[2] + rnorm(1, 0, sigma_phi)

    thresholds <- c(
      min(exp(lpost(theta_mu, frequencies) - lpost(this_theta, frequencies)), 1),
      min(exp(lpost(theta_phi, frequencies) - lpost(this_theta, frequencies)), 1)
    )

    accepts <- runif(2) <= thresholds

    thetas[i, ] <- this_theta
    if (accepts[1]) thetas[i, 1] <- theta_mu[1]
    if (accepts[2]) thetas[i, 2] <- theta_phi[2]

    accepted <- accepted + as.integer(accepts)
  }

  thetas <- thetas[, -0.1 * M]

  names(thetas) <- c("mu", "phi")
  names(accepted) <- c("mu", "phi")
  return(list(theta = thetas, accept_rate = accepted / M))
}
```


```{r}
# Sigma_hat to be used: the proposed one is the one from the Laplace approximation: cov(laplace_samples_Fl)
sd_hat_of_mu <- sqrt(param_cov_mat_Fl[1, 1])
sd_hat_of_phi <- sqrt(param_cov_mat_Fl[2, 2])
M <- 60000
```


```{r Metrop1}
# using starting values as Estimated_mu_Fl and Estimated_phi_Fl that are chosen via MLE
metro_fl1 <- metropolis_algorithm(M,
  theta = c(mu = 3089.94396, phi = 0.399687),
  sd.propositions = c(mu = sd_hat_of_mu, phi = sd_hat_of_phi),
  factors = c(mu = 2.75, phi = 2.75),
  frequencies = Table[1, ]
)
```



```{r}
acceptance_rate_mu_Metrop1 <- metro_fl1$accept_rate["mu"] * 100
cat("Acceptance rate for mu: ", acceptance_rate_mu_Metrop1, "% \n")

acceptance_rate_phi_Metrop1 <- metro_fl1$accept_rate["phi"] * 100
cat("Acceptance rate for phi: ", acceptance_rate_phi_Metrop1, "% \n")
```

```{r}
# Remove bur-in samples
thetasM1 <- tail(metro_fl1$theta, (-0.1 * M))

plot(thetasM1[, 1], thetasM1[, 2], xlab = "mu", ylab = "phi")
library(MASS)
library(plotly)
den3d <- kde2d(thetasM1[, 1], thetasM1[, 2])
plot_ly(x = den3d$x, y = den3d$y, z = den3d$z) %>% add_surface()
```


## (b) diagnostic for convergence

### Graphs analysis {-}
The first, trivial, way of checking if the convergence occurred is looking at the plot of the generated variables:
```{r}
par(mfrow = c(2, 1))
traceplot(as.mcmc(thetasM1))
```
By doing so, one sees that the mixing seems pretty good. For example, the autocorrelation is significative up to not a too high order. As such, the parameter space of $theta$ does not seem to be visited to slowly. 
```{r}
par(mfrow = c(2, 1))
acf((metro_fl1$theta[, 1]), lag.max = 30)
pacf((metro_fl1$theta[, 1]), lag.max = 30)
acf((metro_fl1$theta[, 2]), lag.max = 30)
pacf((metro_fl1$theta[, 2]), lag.max = 30)
```
Since the chains clearly seem to be generated with respect to an Autoregressive process (see ACF/PACF plots), one can compute effectively the effective sample size. It represents how many samples would be generated if there were no autocorrelation. 
```{r}
# out of 48 000 observations
effectiveSize(thetasM1)
```



### Gelman-Rubin {-}

This method requires multiple chains. In this work, the diagnostic will be done with 2 generated chains with two different starting values. As a second chain, more exotic  values (but still possible , a priori) are proposed for initialization: $4000$ for $\mu_1$ and $0.1$ for $\phi_1$. After having run the second chain, the traceplot of both chain is plotted with respect to the first 1000 generations.

```{r}
metro_fl2 <- metropolis_algorithm(60000,
  theta = c(mu = 4000, phi = 0.1),
  sd.propositions = c(mu = sd_hat_of_mu, phi = sd_hat_of_phi),
  factors = c(mu = 2.75, phi = 2.75),
  frequencies = Table[1, ]
)
```

```{r}
par(mfrow = c(2, 1))
traceplot(list(mcmc(metro_fl1$theta[1:2000, ]), mcmc(metro_fl2$theta[1:2000, ])))
```

It seems that convergence occurs rather quickly. After a early generations, the chains seem to have similar mean variance. This can be more formally assessed with the Gelman-Rubin statistic that one wishes to be smaller that, say, $1.1$. Here below is shown the estimated statistic along with its upper bond. 

```{r}
gelman.diag(list(mcmc(metro_fl1$theta[1:2000, ]), mcmc(metro_fl2$theta[1:2000, ])))
```

Those are clearly below, which is a good sign of convergence. To see when convergence may have occurred, one could look at the statistic with respect to the number of generations. Here below are presented the statistics, up to 2000 samples:

```{r}
gelman.plot(list(mcmc(metro_fl1$theta[1:2000, ]), mcmc(metro_fl2$theta[1:2000, ])))
abline(h = 1.1, col = "green")
```

Accordingly, the convergence seems to occurs after roughly 1500 observations for both parameters. 



### Geweke diagnostic {-}

The Geweke diagnostic is useful in the sense that it allows for only one chain to be generated. The chain will be separated into two part: the first accounts for the first $10\%$ of the data while the other accounts for the last $50\%$ of the chain. Under the hypothesis of convergence, the mean of both subsets should be similar. Hence, one can construct a corresponding two means test. The Z-scores are given given here below for $\mu$ and $\phi$, respectively:

```{r}
geweke.diag(mcmc((thetasM1)))
```

To strengthen the belief of convergence, one can successively discard larger numbers of iterations from the beginning of the chain. From there, the z-score is successively computed and presented here below:

```{r}
par(mfrow = c(2, 1))
geweke.plot(mcmc((thetasM1)), nbins = 50)
```

Except for one in $\phi$, the hypothesis of same mean in never rejected. There is still no proof of non convergence of the generated chains. 


## (c) Credible intervals for mu1

```{r}
## FOR mu in Flanders

HPDmu_metrop_Fl <- HPDinterval(as.mcmc((thetasM1)))[1, ]
CImu_Fl <- quantile(thetasM1[, 1], probs = c(alpha / 2, 1 - alpha / 2))
densplot(as.mcmc((thetasM1[, 1])), main = "Comparison between Metropolis and Laplace approximation for mu in Flanders")
legend("topleft",
  legend = c("HPD metropolis", "Quantiles Metropolis", "Laplace HPD", "Laplace quantiles"),
  col = 1:4, lty = c(1, 1, 2, 2)
)
abline(v = c(HPDmu_metrop_Fl[1], HPDmu_metrop_Fl[2]), col = 1)
abline(v = CImu_Fl, col = 2)
abline(v = c(HPDIntervals_Laplace_Fl[1], HPDIntervals_Laplace_Fl[2]), col = 3, lty = 2)
abline(v = c(QuantileCI_Laplace_Fl[1], QuantileCI_Laplace_Fl[2]), col = 4, lty = 2)
```

```{r}
# Encore à faire, là ça fait de la merde
library(kableExtra)
# kable(as.data.frame(matrix(data=cbind(HPDmu_metrop_Fl, CImu_Fl,HPDIntervals_Laplace_Fl, QuantileCI_Laplace_Fl)
#                            , nrow = 2)))
```

# Question 6

```{r}
require(R2WinBUGS)
require(coda)
require(rjags)
```

```{r jags_model_definition}
metro_model <- function() {
  kappa <- 1 / phi
  lambda <- 1 / (phi * mu)

  for (i in 1:10) {
    pi[i] <- pgamma(intervals[i + 1], kappa, lambda) - pgamma(intervals[i], kappa, lambda)
  }

  # Likelihood
  y ~ dmulti(pi, n)

  # Priors
  mu ~ dnorm(3000, pow(306.12, -2))
  phi ~ dunif(0, 10)
}

model.file <- "resources/metropolis.bug"
write.model(metro_model, model.file)
```


```{r jags_model_executionFlanders}
model.file <- "resources/metropolis.bug"

# Test with Flanders
jags_fl <- jags.model(
  file = model.file,
  inits = list(list(mu = 3089.94396, phi = 0.399687)),
  data = list(n = 803, intervals = intervals, y = Table[1, ]),
  n.chains = 1
)

update(jags_fl, 1000)

out_fl <- coda.samples(model = jags_fl, c("mu", "phi"), n.iter = 50000)
out_fl.matrix <- as.matrix(out_fl)
summary(out_fl.matrix)

traceplot(out_fl)
```

```{r}
plot(density(out_fl.matrix[, "mu"]), main = "Comparison R metropolis, Laplace and JAGS distribution for mu", lty = 2)
lines(density(thetasM1[, 1]), col = "red", lty = 2)
lines(density(laplace_samples_Fl[, 1]), col = "blue", lty = 2)

plot(density(out_fl.matrix[, "phi"]), main = "Comparison R metropolis, Laplace and JAGS distribution for phi", lty = 2)
lines(density(thetasM1[, 2]), col = "red", lty = 2)
lines(density(laplace_samples_Fl[, 2]), col = "blue", lty = 2)
```

# Question 7
```{r jags_model_executionWaallonia}
model.file <- "resources/metropolis.bug"

# Test with Wallonia
jags_wal <- jags.model(
  file = model.file,
  inits = list(list(mu = 2914.882, phi = 0.471615)),
  data = list(n = 425, intervals = intervals, y = Table[2, ]),
  n.chains = 1
)

update(jags_wal, 1000)

out_wal <- coda.samples(model = jags_wal, c("mu", "phi"), n.iter = 50000)
out_wal.matrix <- as.matrix(out_wal)
summary(out_wal)
```

```{r}
traceplot(out_wal)
```

# Question 8

The ultimate goal of this work is comparing the households income divergence between Flanders and Wallonia given some a priori and after a experimental study through $1228$ Belgian households. To do so, $50000$ samples have been generated through the metropolis algorithm for Flanders and its French-talking counterpart. Their estimated density is jointly represented here below:
```{r}
plot(density(out_fl.matrix[, "mu"]),
  main = "Comparison between Flanders and Wallonia Net Income households",
  lty = 2, xlim = c(2800, 3500), xlab = "Net Income household (euro)", col = 2, lwd = 2
)
legend("topleft", legend = c("Flanders", "Wallonia"), col = c(2, 4), lwd = 2, lty = c(2, 1))
lines(density(out_wal.matrix[, "mu"]), col = 4, lwd = 2)
```
There is a ```r round( mean(out_fl.matrix[,"mu"]-out_wal.matrix[,"mu"]),2)``` eur difference on average. However, a large part of the density seems to overlap. The density of their difference is shown below along with a $95\%$ qunatile based and HPD credible intervals:
```{r}
library(latex2exp)
diff_of_mu <- out_fl.matrix[, 1] - out_wal.matrix[, 1]
plot(density(diff_of_mu),
  main = "Difference of Net Income between Flanders and Wallonia",
  xlab = TeX("($\\hat{\\mu}_1 - \\hat{\\mu}_2$)"), cex.main = 0.9, lwd = 2
)
legend("topleft", legend = c("95% Quantile-based intervals", "95% HPD intervals"), col = c("red", "blue"), lty = 2:3, cex = 0.7, lwd = 3)
abline(v = quantile(diff_of_mu, probs = c(0.025, 0.975)), col = "red", lty = 2, lwd = 3)
abline(v = HPDinterval(as.mcmc(diff_of_mu)), col = "blue", lty = 3, lwd = 3)
```
On a $95\%$ confidence level, it is impossible to conclude that the average household income level is significantly different between the regions. According to this Bayesian approach, one can only be ```r round((1-ecdf(diff_of_mu)(0)*2)*100 ,2)``` \% sure that the income is different from one another. This is computed by looking from which confidence level one can reject the null hypothesis of same average income.

<!------>
\appendix

# Appendix{#appendix}
## Figures{#figures}
## Code{#code}

\bigskip
\begin{mdframed}[style=thicc, frametitle=Note, frametitlebackgroundcolor=black!30]
  For reproducibility purposes, the complete R project containing the source code and the results is available on \href{https://github.com/AdrienKinart/LSTAT2130BayesianProject}{github.com}.
\end{mdframed}


```{r appendix_pre}
# remotes::install_github('yihui/formatR')
library(formatR)
```

```{r appendix_code, eval=FALSE, echo=TRUE, ref.label=knitr::all_labels(appendix==T)}

```
